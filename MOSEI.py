# -*- coding: utf-8 -*-
"""Dataset_Creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_qMszMY9ILVuMrI7s7ekOV4MET_NxZaT
"""

import os
import pandas as pd
import numpy as np
import torch
from torch.utils import data
import librosa

def load_audio(audiofile, sr):
    y, sr = librosa.load(audiofile, sr=sr)
    return y, sr

def get_mfccs(y, sr):
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=25)
    return mfcc
"""
def get_mfccs(y, sr, target_frames=156):
    hop_length = int(len(y) / target_frames)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=25, hop_length=hop_length)
    if mfcc.shape[1] > target_frames:
        mfcc = mfcc[:, :target_frames]
    elif mfcc.shape[1] < target_frames:
        mfcc = np.pad(mfcc, ((0, 0), (0, target_frames - mfcc.shape[1])), mode='constant')
    return mfcc
"""
def make_dataset(subset, csv_path, bert_output_path):
    # Load CSV into a DataFrame
    df = pd.read_csv(csv_path,skiprows=range(1, 201), nrows=200)

    # Load precomputed DistilBERT outputs
    bert_outputs = np.load(bert_output_path)

    # Filter the DataFrame based on the 'mode' column to match the specified subset
    df_subset = df[df['mode'] == subset]

    # Define label mapping for text labels
    label_mapping = {
        'Positive': 1,
        'Negative': 0,
        'Neutral': 2
    }

    dataset = []
    for idx, row in df_subset.iterrows():
        video_id = row['video_id']
        clip_id = f"{row['clip_id']}.mp3"
        label = label_mapping[row['annotation']]

        # Get precomputed DistilBERT output
        bert_output = bert_outputs[idx]

        encoding_dict = {
            'bert_output': bert_output,
            'clip_id': clip_id,
            'video_id': video_id,
            'label': label
        }
        dataset.append(encoding_dict)

    return dataset

class MOSEI(data.Dataset):
    def __init__(self, csv_path, subset, root_audio_folder, bert_output_path, data_type='audiotext', audio_transform=None):
        self.data = make_dataset(subset, csv_path, bert_output_path)
        self.audio_transform = audio_transform
        self.data_type = data_type
        self.root_audio_folder = root_audio_folder

    def __getitem__(self, index):
        target = self.data[index]['label']

        if self.data_type == 'text' or self.data_type == 'audiotext':
            bert_output = self.data[index]['bert_output']
            text_features = torch.tensor(bert_output)

            if self.data_type == 'text':
                return text_features, target
            if self.data_type == 'audio' or self.data_type == 'audiotext':
              clip_id = self.data[index]['clip_id']
              video_id = self.data[index]['video_id']
              audio_path = os.path.join(self.root_audio_folder, video_id, str(clip_id))
              y, sr = load_audio(audio_path, sr=22050)


            if self.audio_transform is not None:
                self.audio_transform.randomize_parameters()
                y = self.audio_transform(y)

            mfcc = get_mfccs(y, sr)
            audio_features = torch.tensor(mfcc)

            if self.data_type == 'audio':
                return audio_features, target

        if self.data_type == 'audiotext':
            return audio_features, text_features, target

    def __len__(self):
        return len(self.data)

# Path to the CSV file
csv_path = '/content/drive/MyDrive/CMU MOSEI/Audio/Raw 4/Raw4-2/train.csv'

# Subset you want to use (e.g., 'train', 'val', 'test')
subset = 'train'

# Root folder where all the audio subfolders are located
root_audio_folder = '/content/drive/MyDrive/CMU MOSEI/Audio/Raw 4/Raw4-2/train1/'
bert_path = '/content/drive/MyDrive/CMU MOSEI/Distil_bert/distt2.npy'

# Create the dataset instance
dataset = MOSEI(csv_path, subset, root_audio_folder,bert_path, data_type='audiotext')

# Example of how to use the DataLoader
dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)

# Iterate through the DataLoader
for batch in dataloader:
    audio_features, text_features, targets = batch
    print(audio_features.shape,text_features.shape,targets)